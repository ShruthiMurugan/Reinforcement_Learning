K-Armed Bandit Algorithm with Epsilon-Greedy Strategy Algorithm: <br/> <br/>

Step 1: Initialization
1.	Define parameters: <br/>
k: Number of arms (or items) available for recommendation. <br/>
epsilon: Exploration probability, which determines how often the algorithm explores 
<br/> <br/>
2.	Initialize tracking arrays: <br/>
arm_pulls: A k size array of zeros to store the number of times each arm has been pulled. <br/>
estimated_values: A k size array of zeros to store the estimated reward value for each arm.
<br/> <br/>

Step 2: Arm Selection (Epsilon-Greedy Strategy)

1.	For each recommendation step, decide whether to explore or exploit based on epsilon:<br/>
Generate a random number between 0 and 1.<br/>
If the random number is less than epsilon, explore by selecting a random arm.<br/>
Otherwise, exploit by selecting the arm with the highest estimated reward in estimated_values.<br/><br/>

Step 3: Reward Observation and Update<br/>
1.	After selecting an arm:<br/>
Simulate observing a reward based on the armâ€™s true probability. This is done using a binary reward (0 or 1) generated by a Bernoulli trial.<br/>
2.	Update the estimates for the selected arm:<br/>
Increment the pull count for the selected arm in arm_pulls.<br/>
Calculate the new average for the selected arm's reward using the observed reward.<br/>
Store the updated estimated reward in estimated_values.<br/><br/>

Step 4: Simulation of Rounds<br/>
1.	For a specified number of rounds (n_rounds):<br/>
Repeat Steps 2 and 3 recording the observed reward for each round.<br/>
2.	After all rounds are completed, calculate the average reward received across all rounds as a measure of performance.<br/>


