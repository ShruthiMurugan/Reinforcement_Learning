{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b8a8a7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class KArmedBanditRecommender:\n",
    "    \n",
    "    def __init__(self, k, epsilon=0.1):\n",
    "        self.num_arms = k\n",
    "        self.epsilon = epsilon\n",
    "        self.arm_pulls = np.zeros(k)  # Track the number of pulls for each arm\n",
    "        self.estimated_values = np.zeros(k)  # Estimated reward values for each arm\n",
    "\n",
    "    def select_arm(self):\n",
    "        # Epsilon-greedy strategy to select between exploration and exploitation\n",
    "        if np.random.random() < self.epsilon:  # Exploration: choose a random item\n",
    "            return np.random.choice(self.num_arms)\n",
    "        else:\n",
    "            return np.argmax(self.estimated_values)  # Exploitation: Select item with highest estimated reward\n",
    "\n",
    "    def update_estimate(self, chosen_arm, observed_reward):\n",
    "        # Update pull count and estimated reward for the selected arm\n",
    "        self.arm_pulls[chosen_arm] += 1\n",
    "        \n",
    "        n = self.arm_pulls[chosen_arm]\n",
    "        \n",
    "        current_value = self.estimated_values[chosen_arm]\n",
    "        # Incremental update of the mean reward for the selected arm\n",
    "        self.estimated_values[chosen_arm] += (observed_reward - current_value) / n\n",
    "\n",
    "    def run_simulation(self, n_rounds, true_probabilities):\n",
    "        rewards_list = []\n",
    "        \n",
    "        for _ in range(n_rounds):\n",
    "            # Choose an arm to pull: the item\n",
    "            selected_arm = self.select_arm()\n",
    "            \n",
    "            # Obtain a reward based on the true reward distribution\n",
    "            reward = np.random.binomial(1, true_probabilities[selected_arm])\n",
    "            rewards_list.append(reward)\n",
    "            \n",
    "            # Update estimated value for the selected item\n",
    "            self.update_estimate(selected_arm, reward)\n",
    "        \n",
    "        return rewards_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ea79feea",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5  # Number of arms\n",
    "epsilon = 0.1  # Exploration probability\n",
    "n_rounds = 1000  # Total simulation rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8ac3a6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# True reward probabilities for each arm\n",
    "true_probabilities = [0.1, 0.3, 0.5, 0.8, 0.6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ff5858f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bandit_recommender = KArmedBanditRecommender(k, epsilon)\n",
    "rewards = bandit_recommender.run_simulation(n_rounds, true_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a4f5b081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward over 1000 rounds: 0.762\n"
     ]
    }
   ],
   "source": [
    "average_reward = np.mean(rewards)\n",
    "print(\"Average reward over\", n_rounds, \"rounds:\", average_reward)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
